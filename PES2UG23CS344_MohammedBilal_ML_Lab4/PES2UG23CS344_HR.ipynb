{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQVL4UhEUn7f"
      },
      "source": [
        "**Objective**: In this lab, you will implement and compare manual grid search with scikit-learn's built-in GridSearchCV for hyperparameter tuning. You'll work with multiple classification algorithms and combine them using voting classifiers.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Understand hyperparameter tuning through grid search\n",
        "- Compare manual implementation with built-in functions\n",
        "- Learn to create and evaluate voting classifiers\n",
        "- Work with multiple real-world datasets\n",
        "- Visualize model performance using ROC curves and confusion matrices\n",
        "\n",
        "**Datasets Used**:\n",
        "1. Wine Quality - Predicting wine quality based on chemical properties\n",
        "2. HR Attrition - Predicting employee turnover\n",
        "3. Banknote Authentication - Detecting counterfeit banknotes\n",
        "4. QSAR Biodegradation - Predicting chemical biodegradability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhTLi8QnUxFG"
      },
      "source": [
        "\n",
        "## Part 1: Import Libraries and Setup\n",
        "\n",
        "First, let's import all the necessary libraries for our machine learning pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2OwDmFPCRrMI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, roc_curve,\n",
        "                             confusion_matrix, ConfusionMatrixDisplay, classification_report)\n",
        "# Bypass SSL certificate verification for dataset downloads\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0WVjgxFUwIO"
      },
      "source": [
        "# Models and Parameter Grids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jBkCgnvWRvcK"
      },
      "outputs": [],
      "source": [
        "# Define base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
        "\n",
        "# Parameter grids\n",
        "param_grid_dt = {\n",
        "    'feature_selection__k': [5, 10, 15],   # will adjust later to dataset feature size\n",
        "    'classifier__max_depth': [3, 5, 10, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "param_grid_knn = {\n",
        "    'feature_selection__k': [5, 10, 15],\n",
        "    'classifier__n_neighbors': [3, 5, 7, 11],\n",
        "    'classifier__weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "param_grid_lr = {\n",
        "    'feature_selection__k': [5, 10, 15],\n",
        "    'classifier__C': [0.01, 0.1, 1, 10],\n",
        "    'classifier__penalty': ['l1', 'l2']\n",
        "}\n",
        "# (classifier, param_grid, name)\n",
        "classifiers_to_tune = [\n",
        "    (dt, param_grid_dt, \"Decision Tree\"),\n",
        "    (knn, param_grid_knn, \"KNN\"),\n",
        "    (lr, param_grid_lr, \"Logistic Regression\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Ls-UHJWNU6"
      },
      "source": [
        "## Dataset Loading Functions\n",
        "We'll work with four different datasets to test our algorithms across various domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COsVTdScWkV6"
      },
      "source": [
        "### 3.2 HR Attrition Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yg6n6BKwZwGU"
      },
      "outputs": [],
      "source": [
        "def load_hr_attrition():\n",
        "    \"\"\"Load IBM HR Attrition dataset\"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"HR Attrition dataset not found. Please place 'WA_Fn-UseC_-HR-Employee-Attrition.csv' not found.\")\n",
        "        return None, None, None, None, \"HR Attrition (Failed)\"\n",
        "\n",
        "    # Target: Attrition = Yes (1), No (0)\n",
        "    data['Attrition'] = (data['Attrition'] == 'Yes').astype(int)\n",
        "\n",
        "    # Drop ID-like column\n",
        "    X = data.drop(['EmployeeNumber', 'Attrition'], axis=1, errors='ignore')\n",
        "    y = data['Attrition']\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, stratify=y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"IBM HR Attrition dataset loaded and preprocessed successfully.\")\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Testing set shape: {X_test.shape}\")\n",
        "    return X_train, X_test, y_train, y_test, \"HR Attrition\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ovvQpiIXN7O"
      },
      "source": [
        "## Part 4: Manual Grid Search Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1GJmCWupaGE8"
      },
      "outputs": [],
      "source": [
        "def run_manual_grid_search(X_train, y_train, dataset_name):\n",
        "    \"\"\"Run manual grid search and return best estimators\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RUNNING MANUAL GRID SEARCH FOR {dataset_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    best_estimators = {}\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Adjust parameter grids based on dataset size\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    from itertools import product\n",
        "\n",
        "    for classifier_instance, param_grid, name in classifiers_to_tune:\n",
        "        print(f\"--- Manual Grid Search for {name} ---\")\n",
        "        best_score = -1\n",
        "        best_params = None\n",
        "\n",
        "        # 1. Adjust feature selection k values\n",
        "        adjusted_grid = param_grid.copy()\n",
        "        adjusted_grid['feature_selection__k'] = [\n",
        "            k for k in adjusted_grid['feature_selection__k'] if k <= n_features\n",
        "        ]\n",
        "\n",
        "        # 2. Generate all combinations of hyperparameters\n",
        "        param_combinations = [\n",
        "            dict(zip(adjusted_grid.keys(), values))\n",
        "            for values in product(*adjusted_grid.values())\n",
        "        ]\n",
        "\n",
        "        # 3. Evaluate each parameter combination\n",
        "        for params in param_combinations:\n",
        "            scores = []\n",
        "\n",
        "            for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "                # Build pipeline\n",
        "                pipeline = Pipeline(steps=[\n",
        "                    ('scaler', StandardScaler()),\n",
        "                    ('feature_selection', SelectKBest(f_classif)),\n",
        "                    ('classifier', classifier_instance)\n",
        "                ])\n",
        "\n",
        "                # Set parameters\n",
        "                pipeline.set_params(**params)\n",
        "\n",
        "                # Train\n",
        "                pipeline.fit(X_tr, y_tr)\n",
        "\n",
        "                # Predict probability\n",
        "                y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "                # Evaluate AUC\n",
        "                auc_score = roc_auc_score(y_val, y_pred_proba)\n",
        "                scores.append(auc_score)\n",
        "\n",
        "            mean_auc = np.mean(scores)\n",
        "\n",
        "            if mean_auc > best_score:\n",
        "                best_score = mean_auc\n",
        "                best_params = params\n",
        "\n",
        "        # Create the final pipeline for this classifier\n",
        "        print(\"-\" * 90)\n",
        "        print(f\"Best parameters for {name}: {best_params}\")\n",
        "        print(f\"Best cross-validation AUC: {best_score:.4f}\")\n",
        "\n",
        "        final_pipeline = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('feature_selection', SelectKBest(f_classif)),\n",
        "            ('classifier', classifier_instance)\n",
        "        ])\n",
        "\n",
        "        # Set the best parameters found\n",
        "        final_pipeline.set_params(**best_params)\n",
        "\n",
        "        # Fit the final pipeline on the full training data\n",
        "        final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Store the fully trained best pipeline\n",
        "        best_estimators[name] = final_pipeline\n",
        "\n",
        "    return best_estimators\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRUAXSXaXUJG"
      },
      "source": [
        "**Understanding the Manual Implementation**:\n",
        "- **Nested Cross-Validation**: For each parameter combination, we perform 5-fold CV\n",
        "- **Pipeline Integration**: Each step (scaling, feature selection, classification) is properly chained\n",
        "- **AUC Scoring**: We use Area Under the ROC Curve as our optimization metric\n",
        "- **Best Model Selection**: The combination with highest mean AUC across folds is selected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-4A_DGpXotU"
      },
      "source": [
        "## Part 5: Built-in Grid Search Implementation\n",
        "\n",
        "Now let's compare our manual implementation with scikit-learn's GridSearchCV.\n",
        "\n",
        "\n",
        "\n",
        "**Advantages of Built-in GridSearchCV**:\n",
        "- **Parallel Processing**: Uses `n_jobs=-1` for faster computation\n",
        "- **Cleaner Code**: Less verbose than manual implementation\n",
        "- **Built-in Features**: Automatic best model selection and scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "edhV1LPEaOdb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "def run_builtin_grid_search(X_train, y_train, dataset_name):\n",
        "    \"\"\"Run built-in grid search and return best estimators\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RUNNING BUILT-IN GRID SEARCH FOR {dataset_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results_builtin = {}\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    for classifier_instance, param_grid, name in classifiers_to_tune:\n",
        "        print(f\"\\n--- GridSearchCV for {name} ---\")\n",
        "\n",
        "        # ✅ Build pipeline: scale → select features → classifier\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('feature_selection', SelectKBest(f_classif)),\n",
        "            ('classifier', classifier_instance)\n",
        "        ])\n",
        "\n",
        "        # ✅ Adjust feature selection range\n",
        "        adjusted_param_grid = param_grid.copy()\n",
        "        if 'feature_selection__k' in adjusted_param_grid:\n",
        "            adjusted_param_grid['feature_selection__k'] = [\n",
        "                min(k, n_features) for k in adjusted_param_grid['feature_selection__k']\n",
        "            ]\n",
        "\n",
        "        # ✅ Stratified K-Fold CV\n",
        "        cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        # ✅ GridSearchCV setup\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=adjusted_param_grid,\n",
        "            cv=cv_splitter,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # ✅ Fit\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # ✅ Save results\n",
        "        results_builtin[name] = {\n",
        "            'best_estimator': grid_search.best_estimator_,\n",
        "            'best_score (CV)': grid_search.best_score_,\n",
        "            'best_params': grid_search.best_params_\n",
        "        }\n",
        "\n",
        "        print(f\"Best params for {name}: {results_builtin[name]['best_params']}\")\n",
        "        print(f\"Best CV score: {results_builtin[name]['best_score (CV)']:.4f}\")\n",
        "\n",
        "    return results_builtin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk7sL-SRXlow"
      },
      "source": [
        "## Part 6: Model Evaluation and Voting Classifiers\n",
        "\n",
        "This function evaluates individual models and creates voting classifiers to combine their predictions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZLlH3BDoabcE"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(X_test, y_test, best_estimators, dataset_name, method_name=\"Manual\"):\n",
        "    \"\"\"Evaluate models and create visualizations\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUATING {method_name.upper()} MODELS FOR {dataset_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Individual model evaluation\n",
        "    print(f\"\\n--- Individual Model Performance ---\")\n",
        "    for name, model in best_estimators.items():\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(f\"  Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "        print(f\"  Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "        print(f\"  F1-Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "        print(f\"  ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "    # Voting Classifier\n",
        "    print(f\"\\n--- {method_name} Voting Classifier ---\")\n",
        "\n",
        "    if method_name == \"Manual\":\n",
        "        # Manual voting implementation\n",
        "        y_pred_votes = []\n",
        "        y_pred_proba_avg = []\n",
        "\n",
        "        for i in range(len(X_test)):\n",
        "            votes = []\n",
        "            probas = []\n",
        "\n",
        "            for name, model in best_estimators.items():\n",
        "                pred = model.predict(X_test.iloc[[i]])[0]\n",
        "                proba = model.predict_proba(X_test.iloc[[i]])[0, 1]\n",
        "                votes.append(pred)\n",
        "                probas.append(proba)\n",
        "\n",
        "            majority_vote = 1 if np.mean(votes) > 0.5 else 0\n",
        "            avg_proba = np.mean(probas)\n",
        "\n",
        "            y_pred_votes.append(majority_vote)\n",
        "            y_pred_proba_avg.append(avg_proba)\n",
        "\n",
        "        y_pred_votes = np.array(y_pred_votes)\n",
        "        y_pred_proba_avg = np.array(y_pred_proba_avg)\n",
        "\n",
        "    else:  # Built-in\n",
        "        # Create VotingClassifier\n",
        "        estimators = [(name, model) for name, model in best_estimators.items()]\n",
        "        voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
        "        voting_clf.fit(X_train, y_train)  # Note: This assumes X_train, y_train are in scope\n",
        "\n",
        "        y_pred_votes = voting_clf.predict(X_test)\n",
        "        y_pred_proba_avg = voting_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Compute voting metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_votes)\n",
        "    precision = precision_score(y_test, y_pred_votes, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred_votes, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred_votes, zero_division=0)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba_avg)\n",
        "\n",
        "    print(f\"Voting Classifier Performance:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}, Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "    # Visualizations\n",
        "    # ROC Curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name, model in best_estimators.items():\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
        "\n",
        "    # Add voting classifier to ROC\n",
        "    fpr_vote, tpr_vote, _ = roc_curve(y_test, y_pred_proba_avg)\n",
        "    plt.plot(fpr_vote, tpr_vote, label=f'Voting (AUC = {auc:.3f})', linewidth=3, linestyle='--')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curves - {dataset_name} ({method_name})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Confusion Matrix for Voting Classifier\n",
        "    plt.subplot(1, 2, 2)\n",
        "    cm = confusion_matrix(y_test, y_pred_votes)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(ax=plt.gca(), cmap=\"Blues\")\n",
        "    plt.title(f'Voting Classifier - {dataset_name} ({method_name})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return y_pred_votes, y_pred_proba_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwzQEU5pZMDI"
      },
      "source": [
        "## Part 7: Complete Pipeline Function\n",
        "\n",
        "This function orchestrates the entire experiment for each dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jGPXOfWaaf_n"
      },
      "outputs": [],
      "source": [
        "def run_complete_pipeline(dataset_loader, dataset_name):\n",
        "    \"\"\"Run complete pipeline for a dataset\"\"\"\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"PROCESSING DATASET: {dataset_name.upper()}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "\n",
        "    # Load dataset\n",
        "    X_train, X_test, y_train, y_test, actual_name = dataset_loader()\n",
        "    if X_train is None:\n",
        "        print(f\"Skipping {dataset_name} due to loading error.\")\n",
        "        return\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Part 1: Manual Implementation\n",
        "    manual_estimators = run_manual_grid_search(X_train, y_train, actual_name)\n",
        "    manual_votes, manual_proba = evaluate_models(X_test, y_test, manual_estimators, actual_name, \"Manual\")\n",
        "\n",
        "    # Part 2: Built-in Implementation\n",
        "    builtin_results = run_builtin_grid_search(X_train, y_train, actual_name)\n",
        "    builtin_estimators = {name: results['best_estimator']\n",
        "                         for name, results in builtin_results.items()}\n",
        "    builtin_votes, builtin_proba = evaluate_models(X_test, y_test, builtin_estimators, actual_name, \"Built-in\")\n",
        "\n",
        "    print(f\"\\nCompleted processing for {actual_name}\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luYsJ4GtZbz7"
      },
      "source": [
        "## Part 8: Execute the Complete Lab\n",
        "\n",
        "Now let's run our pipeline on all four datasets!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMJ-Bv4fblUn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QwgQCoI1ZcSa",
        "outputId": "2c25c185-4a5f-4ac4-f96d-59d2bbffafd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "################################################################################\n",
            "PROCESSING DATASET: HR ATTRITION\n",
            "################################################################################\n",
            "HR Attrition dataset not found. Please place 'WA_Fn-UseC_-HR-Employee-Attrition.csv' not found.\n",
            "Skipping HR Attrition due to loading error.\n",
            "\n",
            "================================================================================\n",
            "ALL DATASETS PROCESSED!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Run Pipeline for All Datasets ---\n",
        "datasets = [\n",
        "\n",
        "    (load_hr_attrition, \"HR Attrition\")\n",
        "\n",
        "]\n",
        "\n",
        "# Run for each dataset\n",
        "for dataset_loader, dataset_name in datasets:\n",
        "    try:\n",
        "        run_complete_pipeline(dataset_loader, dataset_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dataset_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL DATASETS PROCESSED!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
